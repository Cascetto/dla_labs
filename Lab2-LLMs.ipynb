{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f5d0b9d-7980-4d2c-8154-c07a5f8b5525",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this laboratory we will get our hands dirty working with Large Language Models (e.g. GPT and BERT) to do various useful things. I you haven't already, it is highly recommended to:\n",
    "\n",
    "+ Read the [Attention is All you Need](https://arxiv.org/abs/1706.03762) paper, which is the basis for all transformer-based LLMs.\n",
    "+ Watch (and potentially *code along*) with this [Andrej Karpathy video](https://www.youtube.com/watch?v=kCc8FmEb1nY) which shows you how to build an autoregressive GPT model from the ground up.\n",
    "\n",
    "# Exercise 1: Warming Up\n",
    "In this first exercise you will train a *small* autoregressive GPT model for character generation (the one used by Karpathy in his video) to generate text in the style of Dante Aligheri. Use [this file](https://archive.org/stream/ladivinacommedia00997gut/1ddcd09.txt), which contains the entire text of Dante's Inferno (**note**: you will have to delete some introductory text at the top of the file before training). Train the model for a few epochs, monitor the loss, and generate some text at the end of training. Qualitatively evaluate the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd466d3b-cc41-4de3-9f82-3547569909f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735a497d",
   "metadata": {},
   "source": [
    "# Data proeprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757a16ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "\n",
    "class TextDS(Dataset):\n",
    "    def __init__(self, block_size: int, path = expanduser('~') + \"/datasets/commedia.txt\", start: float = 0, stop: float= 0.7) -> None:\n",
    "        super().__init__()\n",
    "        with open(path, 'r') as f:\n",
    "            self.text = f.read()\n",
    "        l = len(self.text)\n",
    "        self.text = self.text[int(start * l): int(stop * l)]\n",
    "        self.vocab = sorted(list(set(self.text)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.stoi = { ch: i for i, ch in enumerate(self.vocab) }\n",
    "        self.itos = { i: ch for i, ch in enumerate(self.vocab) }\n",
    "        self.encode = lambda s: [self.stoi[c] for c in s]\n",
    "        self.decode = lambda l: ''.join([self.itos[i] for i in l])\n",
    "        self.block_size = block_size\n",
    "\n",
    "        self.data = torch.Tensor(self.encode(self.text)).type(torch.LongTensor)\n",
    "\n",
    "    def __len__(self): return len(self.text) - (self.block_size + 1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index: index + self.block_size]\n",
    "        y = self.data[index + 1: index + self.block_size + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ab1880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32]) torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "block_size = 32\n",
    "batch_size = 8\n",
    "train_ds = TextDS(block_size, start=0, stop=0.3)\n",
    "train_dl = DataLoader(train_ds, batch_size, True)\n",
    "val_ds = TextDS(block_size, start=0.7, stop=0.9)\n",
    "val_dl = DataLoader(val_ds, batch_size, True)\n",
    "test_ds = TextDS(block_size, start=0.9, stop=1)\n",
    "train_dl = DataLoader(train_ds, batch_size, True)\n",
    "\n",
    "for x, y in train_dl:\n",
    "    print(x.shape, y.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc73e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    # take in input an embedding\n",
    "\n",
    "    def __init__(self, input_size: int, head_size: int, block_size: int, masked: bool = True) -> None:\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.Q = nn.Linear(input_size, head_size)\n",
    "        self.K = nn.Linear(input_size, head_size)\n",
    "        self.V = nn.Linear(input_size, head_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.d = head_size\n",
    "        self.masked = masked\n",
    "        if self.masked:\n",
    "            self.tril = torch.tril(torch.ones((block_size, block_size))).to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X [B T C]\n",
    "        # B, T, C = X.shape\n",
    "        q = self.Q(X) # [B T D]\n",
    "        k = self.K(X)\n",
    "        v = self.V(X)\n",
    "\n",
    "        qk: torch.Tensor = (q @ k.transpose(-1, -2)) / (self.d ** 0.5) # [B T D] @ [B D T] = [B T T]      \n",
    "        if self.masked:\n",
    "            qk = self.dropout(qk)\n",
    "            qk = qk.masked_fill(self.tril == 0, float('-inf'))\n",
    "        # qk = F.softmax(qk, dim=-1)\n",
    "        # print(\"a\", qk.isnan().any())\n",
    "        return F.softmax(qk, dim=-1) @ v # [B T T] @ [B T D] = [B T D]\n",
    "        \n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size: int, head_size: int, num_heads: int, block_size: int, masked: bool = True) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead(embedding_size, head_size, block_size, masked) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(head_size * num_heads, embedding_size)\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # each head has an output of [B T T], stacking at [B T (T*N)]\n",
    "        concat = torch.cat([head(X) for head in self.heads], dim=-1) # last dimension\n",
    "        out = self.projection(concat)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super(FeedFoward, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_size, num_heads, block_size) -> None:\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        head_size = embedding_size // num_heads\n",
    "        self.mha = MultiHeadAttention(embedding_size, head_size, num_heads, block_size)\n",
    "        self.feed_forward = FeedFoward(embedding_size)\n",
    "        self.ln1 = nn.LayerNorm(embedding_size)\n",
    "        self.ln2 = nn.LayerNorm(embedding_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.mha(self.ln1(X))\n",
    "        X = X + self.feed_forward(self.ln2(X))\n",
    "        return X\n",
    "    \n",
    "class BLM(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_size: int, num_heads: int, block_size: int, num_layers: int) -> None:\n",
    "        super(BLM, self).__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, embedding_size)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(embedding_size, num_heads, block_size) for _ in range(num_layers)]) \n",
    "        self.ln_f = nn.LayerNorm(embedding_size) # final layer norm\n",
    "        self.lm_head = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        T = X.shape[-1]\n",
    "        token_embedding = self.token_embedding_table(X)\n",
    "        position_embedding = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        X = token_embedding + position_embedding\n",
    "        X = self.blocks(X)\n",
    "        X = self.ln_f(X)\n",
    "        X = self.lm_head(X)\n",
    "        return X\n",
    "\n",
    "    def generate(self, X, max_new_token: int):\n",
    "        # NOTE: Assume the input to be a single element, not a batch!\n",
    "        # X is [T]\n",
    "        T = X.shape[-1]\n",
    "        for _ in range(max_new_token):\n",
    "            # use last T tokens\n",
    "            logits = self(X[-T:]) # [T C]\n",
    "            # get the last timestep\n",
    "            logits = logits[-1, :]\n",
    "            # get the distribution of the next element\n",
    "            probs = F.softmax(logits, dim=-1) # softmax over the last dimensiont\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            X = torch.cat((X, next_token), dim=0)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a55677",
   "metadata": {},
   "source": [
    "# Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93b85a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for x, y in tqdm(dataloader, \"Validation: \", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        prediction_logits = model(x)\n",
    "\n",
    "        B, T, C = prediction_logits.shape\n",
    "        prediction_logits = prediction_logits.view(B*T, C)\n",
    "        y = y.view(B*T)\n",
    "\n",
    "        loss += loss_fn(prediction_logits, y).item()\n",
    "        acc += (prediction_logits.argmax(1) == y).float().sum().item()\n",
    "    return loss / len(dataloader), acc / len(dataloader.dataset)\n",
    "\n",
    "def training(model, train_dataloader, validation_dataloader, loss_fn, optimizer, epochs, validation_freq, log):\n",
    "    losses, accs = [], []\n",
    "    for t in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for x, y in tqdm(train_dataloader, f\"Epoch #{t}: \", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            prediction_logits = model(x)\n",
    "\n",
    "            B, T, C = prediction_logits.shape\n",
    "            prediction_logits = prediction_logits.view(B*T, C)\n",
    "            y = y.view(B*T)\n",
    "            \n",
    "            loss = loss_fn(prediction_logits, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if t % validation_freq == 0:\n",
    "            lss, acc = validation(model, validation_dataloader, loss_fn)\n",
    "            losses.append(lss)\n",
    "            accs.append(acc)\n",
    "            log_dict = {\"loss\": lss, \"accuracy\": acc}\n",
    "            if log:\n",
    "                wandb.log(log_dict)\n",
    "\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9852a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_result(losses, accs, freq):\n",
    "    x = [i * freq for i in range(1, len(losses) + 1)]\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n",
    "    ax1.plot(x, losses)\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax2.plot(x, accs)\n",
    "    ax2.set_title(\"Accuracy\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce541515",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "embedding_size = 512\n",
    "num_head = 16\n",
    "num_layers = 6\n",
    "validation_freq = 5\n",
    "blm = BLM(train_ds.vocab_size, embedding_size, num_head, block_size, num_layers).to(device)\n",
    "optimizer = torch.optim.Adam(blm.parameters(), lr=0.001)\n",
    "loss = F.cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96adb432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  che' la diritta via era smarr`cMQVEfEXoRf'laso\"NMDIpPTr\"TPz>FGDpidloDOo>G?XrZlD\n"
     ]
    }
   ],
   "source": [
    "blm.eval()\n",
    "generated_tokens = blm.generate(train_ds[100][0].to(device),max_new_token=50)\n",
    "generated_tokens = generated_tokens.tolist()\n",
    "generated_text = train_ds.decode(generated_tokens)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a410f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "losses, accuracies = training(blm, train_dl, val_dl, loss, optimizer, epochs, validation_freq, False)\n",
    "torch.save(blm.state_dict(), \"lab2.1_blm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42c6f182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blm.load_state_dict(torch.load(\"lab2.1_blm.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55439311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  che' la diritta via era smarra,\n",
      "coma acquastono.r giosade: dinolla me che mi va\n"
     ]
    }
   ],
   "source": [
    "blm.eval()\n",
    "generated_tokens = blm.generate(train_ds[100][0].to(device),max_new_token=50)\n",
    "generated_tokens = generated_tokens.tolist()\n",
    "generated_text = train_ds.decode(generated_tokens)\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68441a09-dfaf-424a-b640-4fc8cea289b5",
   "metadata": {},
   "source": [
    "# Exercise 2: Working with Real LLMs\n",
    "\n",
    "Our toy GPT can only take us so far. In this exercise we will see how to use the [Hugging Face](https://huggingface.co/) model and dataset ecosystem to access a *huge* variety of pre-trained transformer models.\n",
    "\n",
    "## Exercise 2.1: Installation and text tokenization\n",
    "\n",
    "First things first, we need to install the [Hugging Face transformer library](https://huggingface.co/docs/transformers/index):\n",
    "\n",
    "    conda install -c huggingface -c conda-forge transformers\n",
    "    \n",
    "The key classes that you will work with are `GPT2Tokenizer` to encode text into sub-word tokens, and the `GPT2LMHeadModel`. **Note** the `LMHead` part of the class name -- this is the version of the GPT2 architecture that has the text prediction heads attached to the final hidden layer representations (i.e. what we need to **generate** text). \n",
    "\n",
    "Instantiate the `GPT2Tokenizer` and experiment with encoding text into integer tokens. Compare the length of input with the encoded sequence length.\n",
    "\n",
    "**Tip**: Pass the `return_tensors='pt'` argument to the togenizer to get Pytorch tensors as output (instead of lists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af199a6d-1f3a-4b2c-a23f-d697b93c5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d82a46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "text = \"My first GPT2 encoded text\"\n",
    "encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "print(encoded_text.shape) # expected shape [1 X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd05b3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My first GPT2 encoded text\n",
      "My\n",
      " first\n",
      " G\n",
      "PT\n",
      "2\n",
      " encoded\n",
      " text\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(encoded_text[0]))\n",
    "for i in range(encoded_text.shape[1]):\n",
    "    print(tokenizer.decode(encoded_text[0, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1cdc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256, 50256, 50256, 50256, 50256]\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "coded = tokenizer.encode([\"<SOS>\", \"<EOS>\", \"<PAD>\", \"<UNK>\", \"<RANDOM>\"])\n",
    "print(coded)\n",
    "print(tokenizer.decode(coded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a458b725-63c1-49ae-8011-71a9196387b8",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Generating Text\n",
    "\n",
    "There are a lot of ways we can, given a *prompt* in input, sample text from a GPT2 model. Instantiate a pre-trained `GPT2LMHeadModel` and use the [`generate()`](https://huggingface.co/docs/transformers/v4.27.2/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to generate text from a prompt.\n",
    "\n",
    "**Note**: The default inference mode for GPT2 is *greedy* which might not results in satisfying generated text. Look at the `do_sample` and `temperature` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdad9208-cc9e-4750-baa5-f9367e71362a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78d6f226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/home/dl23emacas/miniconda3/lib/python3.10/site-packages/transformers/generation/utils.py:1346: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50256,   198,   464,   717,   640,   314,  2497,   262,   649,  2196,\n",
      "           286,   262,   983,    11,   314,   373,   523,  6568,    13,   314]])\n"
     ]
    }
   ],
   "source": [
    "generated = model.generate()\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d436f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n",
      "The first time I saw the new version of the game, I was so excited. I\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(generated[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b3d00e7-d4db-440f-8702-11118f07b0a4",
   "metadata": {},
   "source": [
    "# Exercise 3: Reusing Pre-trained LLMs (choose one)\n",
    "\n",
    "Choose **one** of the following exercises (well, *at least* one). In each of these you are asked to adapt a pre-trained LLM (`GPT2Model` or `DistillBERT` are two good choices) to a new Natural Language Understanding task. A few comments:\n",
    "\n",
    "+ Since GPT2 is a *autoregressive* model, there is no latent space aggregation at the last transformer layer (you get the same number of tokens out that you give in input). To use a pre-trained model for a classification or retrieval task, you should aggregate these tokens somehow (or opportunistically select *one* to use).\n",
    "\n",
    "+ BERT models (including DistillBERT) have a special [CLS] token prepended to each latent representation in output from a self-attention block. You can directly use this as a representation for classification (or retrieval).\n",
    "\n",
    "+ The first *two* exercises below can probably be done *without* any fine-tuning -- that is, just training a shallow MLP to classify or represent with the appropriate loss function.\n",
    "\n",
    "# Exercise 3.1: Training a Text Classifier (easy)\n",
    "\n",
    "Peruse the [text classification datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:text-classification&sort=downloads). Choose a *moderately* sized dataset and use a LLM to train a classifier to solve the problem.\n",
    "\n",
    "**Note**: A good first baseline for this problem is certainly to use an LLM *exclusively* as a feature extractor and then train a shallow model.\n",
    "\n",
    "# Exercise 3.2: Training a Question Answering Model (harder)\n",
    "\n",
    "Peruse the [multiple choice question answering datasets on Hugging Face](https://huggingface.co/datasets?task_categories=task_categories:multiple-choice&sort=downloads). Chose a *moderately* sized one and train a model to answer contextualized multiple-choice questions. You *might* be able to avoid fine-tuning by training a simple model to *rank* the multiple choices (see margin ranking loss in Pytorch).\n",
    "\n",
    "# Exercise 3.3: Training a Retrieval Model (hardest)\n",
    "\n",
    "The Hugging Face dataset repository contains a large number of [\"text retrieval\" problems](https://huggingface.co/datasets?task_categories=task_categories:text-retrieval&p=1&sort=downloads). These tasks generally require that the model measure *similarity* between text in some metric space -- naively, just a cosine similarity between [CLS] tokens can get you pretty far. Find an interesting retrieval problem and train a model (starting from a pre-trained LLM of course) to solve it.\n",
    "\n",
    "**Tip**: Sometimes identifying the *retrieval* problems in these datasets can be half the challenge. [This dataset](https://huggingface.co/datasets/BeIR/scifact) might be a good starting point.\n",
    "\n",
    "# Exercise 1:\n",
    "## Dataset exploration\n",
    "In qeusto caso uso distilbert, in quanto prevede un token di padding, che facilita l'uso di frasi di lunghezza diversa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "987e8300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/home/dl23emacas/.cache/huggingface/datasets/rotten_tomatoes/default-data_dir=~%2Fdatasets/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd971ff25c04c88a73527fdf2f66885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\", data_dir=\"~/datasets\")\n",
    "model_name = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69872353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "def map_dataset(examples): return tokenizer(examples['text'], return_tensors='pt', padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a62a5e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/dl23emacas/.cache/huggingface/datasets/rotten_tomatoes/default-data_dir=~%2Fdatasets/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-e7e5012ced87c54a.arrow\n",
      "Loading cached processed dataset at /home/dl23emacas/.cache/huggingface/datasets/rotten_tomatoes/default-data_dir=~%2Fdatasets/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-0bd91201817c20fa.arrow\n",
      "Loading cached processed dataset at /home/dl23emacas/.cache/huggingface/datasets/rotten_tomatoes/default-data_dir=~%2Fdatasets/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/cache-86c9364705de6c40.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset['train'][0]['text']\n",
    "tokenized_dataset = dataset.map(map_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a58072",
   "metadata": {},
   "source": [
    "Vediamo i primi e ultimi 10 token di una frase. Ci possiamo aspettare che gli ultimi token, essendo di padding, siano uguali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "98ce8c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101, 1996, 2600, 2003, 16036, 2000, 2022, 1996, 7398, 2301],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]['input_ids'][:10], tokenized_dataset['train'][0]['input_ids'][-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b879382c",
   "metadata": {},
   "source": [
    "Vediamo come appare il token di padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67e253de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07dedff",
   "metadata": {},
   "source": [
    "Qui viene effettuato un po' di preprocessing del dataset, dato che i modelli di huggingface si aspettano un formato particolare dei dati, che corrisponde ad un dizionario conentenente la label della sequenza e la lista dei token che la definisce, padding inclusi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbae9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns(\"text\")\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be394148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': 1,\n",
       " 'input_ids': [101,\n",
       "  1996,\n",
       "  2600,\n",
       "  2003,\n",
       "  16036,\n",
       "  2000,\n",
       "  2022,\n",
       "  1996,\n",
       "  7398,\n",
       "  2301,\n",
       "  1005,\n",
       "  1055,\n",
       "  2047,\n",
       "  1000,\n",
       "  16608,\n",
       "  1000,\n",
       "  1998,\n",
       "  2008,\n",
       "  2002,\n",
       "  1005,\n",
       "  1055,\n",
       "  2183,\n",
       "  2000,\n",
       "  2191,\n",
       "  1037,\n",
       "  17624,\n",
       "  2130,\n",
       "  3618,\n",
       "  2084,\n",
       "  7779,\n",
       "  29058,\n",
       "  8625,\n",
       "  13327,\n",
       "  1010,\n",
       "  3744,\n",
       "  1011,\n",
       "  18856,\n",
       "  19513,\n",
       "  3158,\n",
       "  5477,\n",
       "  4168,\n",
       "  2030,\n",
       "  7112,\n",
       "  16562,\n",
       "  2140,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d9afc",
   "metadata": {},
   "source": [
    "Dopo aver impostato il formato del dataset su quello di pytorch, il resto è identico ad un normale flow di training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e2456bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "bsz = 8\n",
    "train_dl = DataLoader(tokenized_dataset['train'], shuffle=True, batch_size=bsz)\n",
    "val_dl = DataLoader(tokenized_dataset['validation'], shuffle=True, batch_size=bsz)\n",
    "test_dl = DataLoader(tokenized_dataset['test'], shuffle=True, batch_size=bsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df418335",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ea12168",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tokenized_dataset['train'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f41870",
   "metadata": {},
   "source": [
    "Adesso gli input sono dizionari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96889d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inp = {k: v.to(device) for k, v in inp.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed7ccdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = inp.pop(\"labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1005131d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37216d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25eceb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0437, -0.0704],\n",
      "        [-0.0142, -0.1273]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "val = out.logits\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56918ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1/10:   0%|          | 0/1067 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1/10: 100%|██████████| 1067/1067 [06:32<00:00,  2.72it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2/10: 100%|██████████| 1067/1067 [06:32<00:00,  2.72it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3/10: 100%|██████████| 1067/1067 [06:30<00:00,  2.73it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4/10: 100%|██████████| 1067/1067 [06:38<00:00,  2.68it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5/10: 100%|██████████| 1067/1067 [06:38<00:00,  2.68it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6/10: 100%|██████████| 1067/1067 [06:30<00:00,  2.73it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7/10: 100%|██████████| 1067/1067 [06:31<00:00,  2.73it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8/10: 100%|██████████| 1067/1067 [06:33<00:00,  2.71it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9/10: 100%|██████████| 1067/1067 [06:31<00:00,  2.72it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10/10: 100%|██████████| 1067/1067 [06:30<00:00,  2.73it/s]\n",
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6932, device='cuda:0') 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "print(device)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dl, f\"Epoch #{epoch+1}/{epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        for batch in tqdm(val_dl, f\"Epoch #{epoch+1} Validation\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            val_loss += outputs.loss\n",
    "            val_accuracy += (outputs.logits.argmax(1) == batch['labels']).sum().float().item()\n",
    "        print(val_loss / len(val_dl), val_accuracy / len(val_dl.dataset))\n",
    "torch.save(model.state_dict(), \"lab2.1_version_a.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bfe1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"lab2.1_version_a.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b4556e",
   "metadata": {},
   "source": [
    "## Hybrid Implementation\n",
    "In questo caso invece di usare un modello end-to-end per la classificazione nen usiamo uno adatto ad eseguire il solo encoding, usando la sua codifica come input di una testa linear di classificazione. L'aggregazione dell'embedding di tutti gli elementi della sequenza viene fatta per concatenazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc01715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "class SentenceClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes: int = 2) -> None:\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name) # sequence of 512 tokens with embeddings with d=768\n",
    "        self.head = nn.Sequential(nn.Linear(512 * 768, 768), nn.ReLU(), nn.Linear(768, n_classes))\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is expected to be a dictionary with keys {embeddings, mask_ids}\n",
    "        with torch.no_grad():\n",
    "            X = self.encoder(**X).last_hidden_state\n",
    "        return self.head(self.flatten(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5c1a681",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m SentenceClassifier()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m adam \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentenceClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "model = SentenceClassifier().to(device)\n",
    "adam = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a9629b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1/10: 100%|██████████| 1067/1067 [04:00<00:00,  4.43it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4779, device='cuda:0') 0.7776735459662288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.36it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4691, device='cuda:0') 0.7786116322701688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.37it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5618, device='cuda:0') 0.7833020637898687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.37it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4591, device='cuda:0') 0.7964352720450282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.37it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5078, device='cuda:0') 0.797373358348968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.37it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4981, device='cuda:0') 0.798311444652908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.37it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5408, device='cuda:0') 0.7954971857410882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.37it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6787, device='cuda:0') 0.7879924953095685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.36it/s]\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7164, device='cuda:0') 0.7833020637898687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10/10: 100%|██████████| 1067/1067 [04:04<00:00,  4.37it/s]\n",
      "                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9151, device='cuda:0') 0.774859287054409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dl, f\"Epoch #{epoch+1}/{epochs}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch.pop(\"labels\")\n",
    "        outputs = model(batch)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        adam.zero_grad()\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        for batch in tqdm(val_dl, f\"Epoch #{epoch+1} Validation\", leave=False):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = batch.pop(\"labels\")\n",
    "            outputs = model(batch)\n",
    "            val_loss += F.cross_entropy(outputs, labels)\n",
    "            val_accuracy += (outputs.argmax(1) == labels).sum().float().item()\n",
    "        print(val_loss / len(val_dl), val_accuracy / len(val_dl.dataset))\n",
    "torch.save(model.state_dict(), \"lab2.1_version_b.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9fe3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"lab2.1_version_b.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23a6edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
