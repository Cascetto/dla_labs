{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e2ac44-45aa-4d2c-9e16-907bf8683659",
   "metadata": {},
   "source": [
    "# Laboratory #4: Adversarial Learning and OOD Detection\n",
    "\n",
    "In this laboratory session we will develop a methodology for detecting OOD samples and measuring the quality of OOD detection. We will also experiment with incorporating adversarial examples during training to render models more robust to adversarial attacks.\n",
    "\n",
    "---\n",
    "## Exercise 1: OOD Detection and Performance Evaluation\n",
    "In this first exercise you will build a simple OOD detection pipeline and implement some performance metrics to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c02677-b193-4c3b-a5e1-c9c4015b9dc2",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Build a simple OOD detection pipeline\n",
    "\n",
    "Implement an OOD detection pipeline (like in the Flipped Activity notebook) using an ID and an OOD dataset of your choice. Some options:\n",
    "\n",
    "+ CIFAR-10 (ID), Subset of CIFAR-100 (OOD). You will need to wrap CIFAR-100 in some way to select a subset of classes that are *not* in CIFAR-10 (see `torch.utils.data.Subset`).\n",
    "+ Labeled Faces in the Wild (ID), CIFAR-10 or FakeData (OOD). The LfW dataset is available in Scikit-learn (see `sklearn.datasets.fetch_lfw_people`).\n",
    "+ Something else, but if using images keep the images reasonably small!\n",
    "\n",
    "In this exercise your *OOD Detector* should produce a score representing how \"out of distribution\" a test sample is. We will implement some metrics in the next exercise, but for now use the techniques from the flipped activity notebook to judge how well OOD scoring is working (i.e. histograms).\n",
    "\n",
    "**Note**: Make sure you make a validation split of your ID dataset for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06bd79-ac57-473c-9ce0-30c0381ad34f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b28fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100_wrapper(Dataset):\n",
    "\n",
    "    def __init__(self, classes: set[int], get_train: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.classes = classes\n",
    "        cifar = CIFAR100(\"~/datasets\", download=get_train, train=get_train, transform=Compose([ToTensor()]))\n",
    "        samples = []\n",
    "        labels = []\n",
    "        for i in range(len(cifar)):\n",
    "            if cifar[i][1] in classes:\n",
    "                samples.append(cifar[i][0])\n",
    "                labels.append(torch.Tensor([cifar[i][1]]).long())\n",
    "        self.data = torch.stack(samples)\n",
    "        self.labels = torch.cat(labels)\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, torch.Tensor]: return self.data[index], self.labels[index]\n",
    "\n",
    "index_id = set(range(20))\n",
    "index_ood = set(range(30, 40))\n",
    "\n",
    "batch_size=64\n",
    "\n",
    "c20 = CIFAR100_wrapper(index_id)\n",
    "c20_train, c20_val = random_split(c20, [0.7, 0.3])\n",
    "c20_test = CIFAR100_wrapper(index_id)\n",
    "c20_ood = CIFAR100_wrapper(index_ood)\n",
    "dl_train = DataLoader(c20_train, batch_size, shuffle=True)\n",
    "dl_val = DataLoader(c20_val, batch_size, shuffle=True)\n",
    "dl_test = DataLoader(c20_test, batch_size, shuffle=True)\n",
    "dl_ood = DataLoader(c20_ood, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfd807",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ood_test(model, dataloader, threshold = 1.):\n",
    "    max_logits = []\n",
    "    second_guess = []\n",
    "    for x, _ in dataloader:\n",
    "        x = x.to(device)\n",
    "        logits_sorted, _ = torch.sort(model(x), descending=True)\n",
    "        max_logits.append(logits_sorted[:, 0])\n",
    "        second_guess.append(logits_sorted[:, 1])\n",
    "    max_logits = torch.cat(max_logits)\n",
    "    second_guess = torch.cat(second_guess)\n",
    "    return (max_logits - second_guess) > threshold\n",
    "\n",
    "@torch.no_grad()\n",
    "def ood_score(model, dataloader):\n",
    "    max_logits = []\n",
    "    second_guess = []\n",
    "    for x, _ in dataloader:\n",
    "        x = x.to(device)\n",
    "        logits_sorted, _ = torch.sort(model(x), descending=True)\n",
    "        max_logits.append(logits_sorted[:, 0])\n",
    "        second_guess.append(logits_sorted[:, 1])\n",
    "    max_logits = torch.cat(max_logits)\n",
    "    second_guess = torch.cat(second_guess)\n",
    "    return (max_logits - second_guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90395410",
   "metadata": {},
   "source": [
    "# Model (note for me: OK)\n",
    "A simple model for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b27afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.head = nn.Sequential(nn.Linear(784*2, 1000), nn.ReLU(), nn.Linear(1000, n_classes))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.dropout1(self.conv1(X)))\n",
    "        X = self.pool1(X)\n",
    "        X = F.relu(self.dropout2(self.conv2(X)))\n",
    "        X = self.pool2(X)\n",
    "        X = self.flatten(X)\n",
    "        return self.head(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf16b60",
   "metadata": {},
   "source": [
    "# CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar = CIFAR10(\"~/datasets\", transform=Compose([ToTensor() # ,\n",
    "     # Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "     ]))\n",
    "c_train, c_val = random_split(cifar, [0.7, 0.3])\n",
    "dl_train = DataLoader(c_train, batch_size=batch_size, shuffle=True)\n",
    "dl_val = DataLoader(c_val, batch_size=batch_size, shuffle=True)\n",
    "dl_test = DataLoader(CIFAR10(\"~/datasets\", train=False, transform=ToTensor()), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e576536",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "res = 32\n",
    "i_channels = 3\n",
    "e_channels = 32\n",
    "n_convs = 16\n",
    "# model = RCNN(res, i_channels, e_channels, n_convs, n_classes).to(device)\n",
    "model = CifarClassifier(n_classes).to(device)\n",
    "x = torch.rand((16, 3, 32, 32)).to(device)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331afd1",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff782efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validation(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for x, y in tqdm(dataloader, \"Validation: \", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        prediction_logits = model(x)\n",
    "        loss += loss_fn(prediction_logits, y).item()\n",
    "        acc += (prediction_logits.argmax(1) == y).float().sum().item()\n",
    "    return loss / len(dataloader), acc / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d23b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_dataloader, validation_dataloader, loss_fn, optimizer, epochs, validation_freq, log):\n",
    "    losses, accs = [], []\n",
    "    for t in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in tqdm(train_dataloader, f\"Epoch #{t}: \", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            prediction_logits = model(x)\n",
    "            loss = loss_fn(prediction_logits, y)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if t % validation_freq == 0:\n",
    "            lss, acc = validation(model, validation_dataloader, loss_fn)\n",
    "            print(f\"Loss Train = {train_loss / len(train_dataloader)}, Loss Val = {lss}, Acc = {acc}\")\n",
    "            losses.append(lss)\n",
    "            accs.append(acc)\n",
    "            log_dict = {\"loss\": lss, \"accuracy\": acc}\n",
    "            if log:\n",
    "                wandb.log(log_dict)\n",
    "\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "epochs = 30\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lss, accs = training(model, dl_train, dl_val, loss_fn, optim, epochs, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d05e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_data = ood_score(model, dl_test)\n",
    "ood_data = ood_score(model, dl_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c03433",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(id_data.cpu(), bins=30, density=True, alpha=0.5, label=\"ID\")\n",
    "plt.hist(ood_data.cpu(), bins=30, density=True, alpha=0.5, label=\"OOD\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3878924-66ec-44bc-80af-4ce9b7e46ef2",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Measure your OOD detection performance\n",
    "\n",
    "There are several metrics used to evaluate OOD detection performance, we will concentrate on two threshold-free approaches: the area under the Receiver Operator Characteristic (ROC) curve for ID classification, and the area under the Precision-Recall curve for *both* ID and OOD scoring. See [the ODIN paper](https://arxiv.org/pdf/1706.02690.pdf) section 4.3 for a description of OOD metrics.\n",
    "\n",
    "Use the functions in `sklearn.metrics` to produce ROC and PR curves for your OOD detector. Some useful functions:\n",
    "\n",
    "+ [`sklearn.metric.RocCurveDisplay.from_predictions`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.RocCurveDisplay.html)\n",
    "+ [`sklearn.metrics.PrecisionRecallDisplay`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.PrecisionRecallDisplay.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95872246-85fb-41a5-8018-8643d76bfcb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, precision_score, recall_score\n",
    "@torch.no_grad()\n",
    "def ood_roc_test(model, dataloader_id, dataloader_ood, threshold = 0.4):\n",
    "    predictions = torch.cat((ood_test(model, dataloader_id, threshold), ood_test(model, dataloader_ood, threshold)), dim=0)\n",
    "    true_values = torch.cat((torch.ones(len(dataloader_id.dataset)), torch.zeros(len(dataloader_ood.dataset))), dim=0) \n",
    "    # print(len(dataloader_ood.dataset), len(dataloader_id.dataset), len(true_values), len(predictions))\n",
    "    RocCurveDisplay.from_predictions(true_values, predictions.cpu())\n",
    "    PrecisionRecallDisplay.from_predictions(true_values, predictions.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_roc_test(model, dl_test, dl_ood)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00e7265-77c4-4659-9bda-cc23247370bd",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 2: Enhancing Robustness to Adversarial Attack\n",
    "\n",
    "In this second exercise we will experiment with enhancing our base model to be (more) robust to adversarial attacks. \n",
    "\n",
    "### Exercise 2.1: Implement FGSM and generate adversarial examples\n",
    "\n",
    "Recall that the Fast Gradient Sign Method (FGSM) perturbs samples in the direction of the gradient with respect to the input $\\mathbf{x}$:\n",
    "$$ \\boldsymbol{\\eta}(\\mathbf{x}) = \\varepsilon \\mathrm{sign}(\\nabla_{\\mathbf{x}} \\mathcal{L}(\\boldsymbol{\\theta}, \\mathbf{x}, y)) ) $$\n",
    "Implement FGSM and generate some *adversarial examples* using your trained ID model. Evaluate these samples qualitatively and quantitatively. Evaluate how dependent on $\\varepsilon$ the quality of these samples are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840eaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = torch.randn((10)) * 10\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fa9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = r.sign()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c052cbf-c45e-4d03-805e-53b917137043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "def fgs_perturbation(model: torch.nn.Module, batch: torch.Tensor, batch_label: torch.Tensor, loss_fn, epsilon: float = 0.01):\n",
    "    # gradient wrt input\n",
    "    batch.requires_grad = True\n",
    "    model.requires_grad_ = False\n",
    "    logits = model(batch)\n",
    "    loss = loss_fn(logits, batch_label)\n",
    "    loss.backward()\n",
    "    perturbation = batch.grad.data # gradient\n",
    "    perturbation = perturbation.sign() # signed gradient\n",
    "    perturbation = epsilon * perturbation # scaled perturbation\n",
    "    return perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016957bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dl_train:\n",
    "    batch, batch_labels = x, y\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2606a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[0].transpose(0, 1).transpose(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = 4\n",
    "fig, axes = plt.subplots(2, cols, figsize=(16, 6))\n",
    "for i in range(cols):\n",
    "    x, y = cifar[i][0].to(device), torch.tensor([cifar[i][1]]).to(device)\n",
    "    x = x.view(1, *x.shape)\n",
    "    pert = fgs_perturbation(model, x, y, F.cross_entropy)\n",
    "    perturbed = x + pert\n",
    "    perturbed = perturbed[0] \n",
    "    x = x[0]\n",
    "    axes[0][i].imshow(x.transpose(0, 1).transpose(1, 2).detach().cpu().numpy())\n",
    "    axes[1][i].imshow(perturbed.transpose(0, 1).transpose(1, 2).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7990240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, cols, figsize=(16, 6))\n",
    "epsilons = [0.01, 0.02, 0.05, 0.1]\n",
    "for i in range(cols):\n",
    "    x, y = cifar[i][0].to(device), torch.tensor([cifar[i][1]]).to(device)\n",
    "    x = x.view(1, *x.shape)\n",
    "    pert = fgs_perturbation(model, x, y, F.cross_entropy, epsilon=epsilons[i])\n",
    "    perturbed = x + pert\n",
    "    perturbed = perturbed[0] \n",
    "    x = x[0]\n",
    "    axes[0][i].imshow(x.transpose(0, 1).transpose(1, 2).detach().cpu().numpy())\n",
    "    axes[1][i].imshow(perturbed.transpose(0, 1).transpose(1, 2).detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3708188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_pertrubated(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    for x, y in tqdm(dataloader, \"Validation: \", leave=False):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pert = fgs_perturbation(model, x, y, loss_fn, 0.)\n",
    "        x = x + pert\n",
    "        prediction_logits = model(x)\n",
    "        loss += loss_fn(prediction_logits, y).item()\n",
    "        acc += (prediction_logits.argmax(1) == y).float().sum().item()\n",
    "    return loss / len(dataloader), acc / len(dataloader.dataset)\n",
    "\n",
    "validation_pertrubated(model, dl_test, F.cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394477b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d82e7dc-cdfe-4201-8dc5-3891dfd6ea49",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Augment training with adversarial examples\n",
    "\n",
    "Use your implementation of FGSM to augment your training dataset with adversarial samples. Ideally, you should implement this data augmentation *on the fly* so that the adversarial samples are always generated using the current model. Evaluate whether the model is more (or less) robust to ID samples using your OOD detection pipeline and metrics you implemented in Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da5e2d-30a9-48d6-86fd-6711612eb976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "def augment_batch(model, batch, batch_labels, loss, epsilon=0.01):\n",
    "    pert = fgs_perturbation(model, batch, batch_labels, loss, epsilon)\n",
    "    augmented_data = batch + pert\n",
    "    batch = torch.cat([batch, augmented_data], dim=0)\n",
    "    batch_labels = torch.cat([batch_labels, batch_labels], dim=0)\n",
    "    return batch, batch_labels\n",
    "\n",
    "batch, batch_labels = batch.to(device), batch_labels.to(device)\n",
    "aug, aug_l = augment_batch(model, batch, batch_labels, F.cross_entropy)\n",
    "print(batch.shape, aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cca370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE  l'augment fatto in questo modo implica che il batch size effettivo è doppio rispetto a quello dichiarato (=64) \n",
    "def training_with_augment(model, train_dataloader, validation_dataloader, loss_fn, optimizer, epochs, validation_freq, log):\n",
    "    losses, accs = [], []\n",
    "    for t in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in tqdm(train_dataloader, f\"Epoch #{t}: \", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            x, y = augment_batch(model, x, y, loss_fn)\n",
    "            prediction_logits = model(x)\n",
    "            loss = loss_fn(prediction_logits, y)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if t % validation_freq == 0:\n",
    "            lss, acc = validation(model, validation_dataloader, loss_fn)\n",
    "            print(f\"Loss Train = {train_loss / len(train_dataloader)}, Loss Val = {lss}, Acc = {acc}\")\n",
    "            losses.append(lss)\n",
    "            accs.append(acc)\n",
    "            log_dict = {\"loss\": lss, \"accuracy\": acc}\n",
    "            if log:\n",
    "                wandb.log(log_dict)\n",
    "\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dea9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy\n",
    "model = CifarClassifier(n_classes).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lss, accs = training_with_augment(model, dl_train, dl_val, loss_fn, optim, epochs, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49123e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_roc_test(model, dl_test, dl_ood, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa8666b-85fc-4e83-b8d4-fe6a5dff0fea",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercise 3: Wildcard\n",
    "\n",
    "You know the drill. Pick *ONE* of the following exercises to complete.\n",
    "\n",
    "### Exercise 3.1: Implement ODIN for OOD detection\n",
    "ODIN is a very simple approach, and you can already start experimenting by implementing a temperature hyperparameter in your base model and doing a grid search on $T$ and $\\varepsilon$.\n",
    "\n",
    "### Exercise 3.2: Implement JARN\n",
    "In exercise 2.2 you already implemented Jacobian-regularized learning to make your model more robust to adversarial samples. Add a *discriminator* to your model to encourage the adversarial samples used for regularization to be more *salient*.\n",
    "\n",
    "See [the JARN paper](https://arxiv.org/abs/1912.10185) for more details.\n",
    "\n",
    "### Exercise 3.3: Experiment with *targeted* adversarial attacks\n",
    "Implement the targeted Fast Gradient Sign Method to generate adversarial samples that *imitate* samples from a specific class. Evaluate your adversarial samples qualitatively and quantitatively.\n",
    "\n",
    "## Exercise 3.1: Implement ODIN for OOD detection\n",
    "ODIN is a very simple approach, and you can already start experimenting by implementing a temperature hyperparameter in your base model and doing a grid search on $T$ and $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarClassifierWithODIN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.head = nn.Sequential(nn.Linear(784*2, 1000), nn.ReLU(), nn.Linear(1000, n_classes))\n",
    "\n",
    "    def forward(self, X, T = 1):\n",
    "        X = F.relu(self.dropout1(self.conv1(X)))\n",
    "        X = self.pool1(X)\n",
    "        X = F.relu(self.dropout2(self.conv2(X)))\n",
    "        X = self.pool2(X)\n",
    "        X = self.flatten(X)\n",
    "        return self.head(X) / T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e70185ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "\n",
    "def odin_perturbation(model: torch.nn.Module, batch: torch.Tensor, epsilon: float = 0.01):\n",
    "    # gradient wrt input\n",
    "    batch.requires_grad = True\n",
    "    model.requires_grad_ = False\n",
    "    logits = model(batch)\n",
    "    probabilities, _ = torch.max(F.log_softmax(logits, dim=1), dim=1)\n",
    "    probabilities.backward()\n",
    "\n",
    "    perturbation = -batch.grad.data # gradient\n",
    "    perturbation = perturbation.sign() # signed gradient\n",
    "    perturbation = epsilon * perturbation # scaled perturbation\n",
    "    return perturbation\n",
    "\n",
    "def odin_augment(model, batch, epsilon=0.01):\n",
    "    pert = odin_perturbation(model, batch, epsilon)\n",
    "    augmented_data = batch - pert\n",
    "    # batch = torch.cat([batch, augmented_data], dim=0)\n",
    "    # batch_labels = torch.cat([batch_labels, batch_labels], dim=0)\n",
    "    return augmented_data\n",
    "\n",
    "def odin_training(model, train_dataloader, validation_dataloader, loss_fn, optimizer, epochs, validation_freq, log):\n",
    "    losses, accs = [], []\n",
    "    for t in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, y in tqdm(train_dataloader, f\"Epoch #{t}: \", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # x = odin_augment(model, x)\n",
    "            prediction_logits = model(x)\n",
    "            loss = loss_fn(prediction_logits, y)\n",
    "            train_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if t % validation_freq == 0:\n",
    "            lss, acc = validation(model, validation_dataloader, loss_fn)\n",
    "            print(f\"Loss Train = {train_loss / len(train_dataloader)}, Loss Val = {lss}, Acc = {acc}\")\n",
    "            losses.append(lss)\n",
    "            accs.append(acc)\n",
    "            log_dict = {\"loss\": lss, \"accuracy\": acc}\n",
    "            if log:\n",
    "                wandb.log(log_dict)\n",
    "\n",
    "    return losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87e7f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ood_odin_classication(model, dataloader, temperature, threshold = 1.):\n",
    "    probs = []\n",
    "    for x, _ in dataloader:\n",
    "        x = x.to(device)\n",
    "        x = odin_augment(model, x)\n",
    "        prob, _ = torch.max(F.softmax(model(x, temperature), dim=1), dim=1)\n",
    "        probs.append(prob)\n",
    "    probs = torch.cat(probs, dim=0)\n",
    "    return probs > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6d9a6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, precision_score, recall_score\n",
    "def ood_odin_test(model, dataloader_id, dataloader_ood, temperature, threshold):\n",
    "    print(temperature, threshold)\n",
    "    predictions = torch.cat((ood_odin_classication(model, dataloader_id, temperature, threshold), ood_odin_classication(model, dataloader_ood, temperature, threshold)), dim=0)\n",
    "    true_values = torch.cat((torch.ones(len(dataloader_id.dataset)), torch.zeros(len(dataloader_ood.dataset))), dim=0) \n",
    "    # print(len(dataloader_ood.dataset), len(dataloader_id.dataset), len(true_values), len(predictions))\n",
    "    RocCurveDisplay.from_predictions(true_values, predictions.cpu(), name=f\"T={temperature}, eps={threshold}\")\n",
    "    PrecisionRecallDisplay.from_predictions(true_values, predictions.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2eb838a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:   3%|▎         | 14/547 [00:00<00:07, 70.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train = 0.9652720598241746, Loss Val = 1.296831797031646, Acc = 0.6295333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train = 0.6325177921891431, Loss Val = 1.0976339872847212, Acc = 0.6598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train = 0.41761843969115825, Loss Val = 1.0194116835898541, Acc = 0.6607333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train = 0.30468308162863555, Loss Val = 1.0124065690852226, Acc = 0.6502666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train = 0.2384546856162644, Loss Val = 1.0356932023738292, Acc = 0.6420666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Train = 0.20510958641140944, Loss Val = 1.01833019713138, Acc = 0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1.296831797031646,\n",
       "  1.0976339872847212,\n",
       "  1.0194116835898541,\n",
       "  1.0124065690852226,\n",
       "  1.0356932023738292,\n",
       "  1.01833019713138],\n",
       " [0.6295333333333333,\n",
       "  0.6598,\n",
       "  0.6607333333333333,\n",
       "  0.6502666666666667,\n",
       "  0.6420666666666667,\n",
       "  0.653])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odin_model = CifarClassifierWithODIN(n_classes).to(device)\n",
    "optimizer = torch.optim.Adam(odin_model.parameters(), lr=0.001)\n",
    "loss = F.cross_entropy\n",
    "temperatures = [0.1 ** i for i in range(-3, 4)]\n",
    "epsilons = [0.1 *+ i for i in range(4)]\n",
    "training(odin_model, dl_train, dl_val, loss, optimizer, epochs, 5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "69a53d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999.9999999999999 0.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m T \u001b[39min\u001b[39;00m temperatures:\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m eps \u001b[39min\u001b[39;00m epsilons:\n\u001b[0;32m----> 3\u001b[0m         ood_odin_test(odin_model, dl_test, dl_ood, T, eps)\n",
      "Cell \u001b[0;32mIn[90], line 5\u001b[0m, in \u001b[0;36mood_odin_test\u001b[0;34m(model, dataloader_id, dataloader_ood, temperature, threshold)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mood_odin_test\u001b[39m(model, dataloader_id, dataloader_ood, temperature, threshold):\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(temperature, threshold)\n\u001b[0;32m----> 5\u001b[0m     predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((ood_odin_classication(model, dataloader_id, temperature, threshold), ood_odin_classication(model, dataloader_ood, temperature, threshold)), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m      6\u001b[0m     true_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((torch\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(dataloader_id\u001b[39m.\u001b[39mdataset)), torch\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(dataloader_ood\u001b[39m.\u001b[39mdataset))), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \n\u001b[1;32m      7\u001b[0m     \u001b[39m# print(len(dataloader_ood.dataset), len(dataloader_id.dataset), len(true_values), len(predictions))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[89], line 5\u001b[0m, in \u001b[0;36mood_odin_classication\u001b[0;34m(model, dataloader, temperature, threshold)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m x, _ \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      4\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m     x \u001b[39m=\u001b[39m odin_augment(model, x)\n\u001b[1;32m      6\u001b[0m     prob, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(F\u001b[39m.\u001b[39msoftmax(model(x, temperature), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     probs\u001b[39m.\u001b[39mappend(prob)\n",
      "Cell \u001b[0;32mIn[96], line 17\u001b[0m, in \u001b[0;36modin_augment\u001b[0;34m(model, batch, epsilon)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39modin_augment\u001b[39m(model, batch, epsilon\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     pert \u001b[39m=\u001b[39m odin_perturbation(model, batch, epsilon)\n\u001b[1;32m     18\u001b[0m     augmented_data \u001b[39m=\u001b[39m batch \u001b[39m-\u001b[39m pert\n\u001b[1;32m     19\u001b[0m     \u001b[39m# batch = torch.cat([batch, augmented_data], dim=0)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m# batch_labels = torch.cat([batch_labels, batch_labels], dim=0)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[96], line 9\u001b[0m, in \u001b[0;36modin_perturbation\u001b[0;34m(model, batch, epsilon)\u001b[0m\n\u001b[1;32m      7\u001b[0m logits \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m      8\u001b[0m probabilities, _ \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(F\u001b[39m.\u001b[39mlog_softmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m probabilities\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     11\u001b[0m perturbation \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mbatch\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata \u001b[39m# gradient\u001b[39;00m\n\u001b[1;32m     12\u001b[0m perturbation \u001b[39m=\u001b[39m perturbation\u001b[39m.\u001b[39msign() \u001b[39m# signed gradient\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:193\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    189\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[1;32m    190\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    192\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 193\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:88\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "for T in temperatures:\n",
    "    for eps in epsilons:\n",
    "        ood_odin_test(odin_model, dl_test, dl_ood, T, eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09966b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
