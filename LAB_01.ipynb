{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {},
   "source": [
    "# Deep Learning Applications: Laboratory #1\n",
    "\n",
    "In this first laboratory we will work relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch, but as I said in the introductory lecture: please feel free to use and experiment with whatever tools you like.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {},
   "source": [
    "## Exercise 1: Warming Up\n",
    "In this series of exercises I want you to try to duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "We will do this in steps using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "Recall that the main message of the ResNet paper is that **deeper** networks do not **guarantee** more reduction in training loss (or in validation accuracy). Below you will incrementally build a sequence of experiments to verify this for an MLP. A few guidelines:\n",
    "\n",
    "+ I have provided some **starter** code at the beginning. **NONE** of this code should survive in your solutions. Not only is it **very** badly written, it is also written in my functional style that also obfuscates what it's doing (in part to **discourage** your reuse!). It's just to get you *started*.\n",
    "+ These exercises ask you to compare **multiple** training runs, so it is **really** important that you factor this into your **pipeline**. Using [Tensorboard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) is a **very** good idea -- or, even better [Weights and Biases](https://wandb.ai/site).\n",
    "+ You may work and submit your solutions in **groups of at most two**. Share your ideas with everyone, but the solutions you submit *must be your own*.\n",
    "\n",
    "First some boilerplate to get you started, then on to the actual exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b6d1-3df0-464c-9a5f-8c611257a971",
   "metadata": {},
   "source": [
    "### Preface: Some code to get you started\n",
    "\n",
    "What follows is some **very simple** code for training an MLP on MNIST. The point of this code is to get you up and running (and to verify that your Python environment has all needed dependencies).\n",
    "\n",
    "**Note**: As you read through my code and execute it, this would be a good time to think about *abstracting* **your** model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with some standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "Here is some basic dataset loading, validation splitting code to get you started working with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272a69db-0416-444a-9be4-5f055ff48bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard MNIST transform.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST train and test.\n",
    "ds_train = MNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "ds_test = MNIST(root='./datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split train into train and validation.\n",
    "val_size = 5000\n",
    "I = np.random.permutation(len(ds_train))\n",
    "ds_val = Subset(ds_train, I[:val_size])\n",
    "ds_train = Subset(ds_train, I[val_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e05e96-7707-4490-98b8-50cb5e330af1",
   "metadata": {},
   "source": [
    "#### Boilerplate training and evaluation code\n",
    "\n",
    "This is some **very** rough training, evaluation, and plotting code. Again, just to get you started. I will be *very* disappointed if any of this code makes it into your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcce348-f603-4d57-b9a8-5b1c6eba28ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Function to train a model for a single epoch over the data loader.\n",
    "def train_epoch(model, dl, opt, epoch='Unknown', device='cpu'):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for (xs, ys) in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n",
    "        xs = xs.to(device)\n",
    "        ys = ys.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(xs)\n",
    "        loss = F.cross_entropy(logits, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Function to evaluate model over all samples in the data loader.\n",
    "def evaluate_model(model, dl, device='cpu'):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    gts = []\n",
    "    for (xs, ys) in tqdm(dl, desc='Evaluating', leave=False):\n",
    "        xs = xs.to(device)\n",
    "        preds = torch.argmax(model(xs), dim=1)\n",
    "        gts.append(ys)\n",
    "        predictions.append(preds.detach().cpu().numpy())\n",
    "        \n",
    "    # Return accuracy score and classification report.\n",
    "    return (accuracy_score(np.hstack(gts), np.hstack(predictions)),\n",
    "            classification_report(np.hstack(gts), np.hstack(predictions), zero_division=0, digits=3))\n",
    "\n",
    "# Simple function to plot the loss curve and validation accuracy.\n",
    "def plot_validation_curves(losses_and_accs):\n",
    "    losses = [x for (x, _) in losses_and_accs]\n",
    "    accs = [x for (_, x) in losses_and_accs]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Average Training Loss per Epoch')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(accs)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'Best Accuracy = {np.max(accs)} @ epoch {np.argmax(accs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875008c3-306c-4e39-a845-d7bda7862621",
   "metadata": {},
   "source": [
    "#### A basic, parameterized MLP\n",
    "\n",
    "This is a very basic implementation of a Multilayer Perceptron. Don't waste too much time trying to figure out how it works -- the important detail is that it allows you to pass in a list of input, hidden layer, and output *widths*. **Your** implementation should also support this for the exercises to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1e503a-37df-4fb9-94e7-85d0adb494bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(nin, nout) for (nin, nout) in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return reduce(lambda f, g: lambda x: g(F.relu(f(x))), self.layers, lambda x: x.flatten(1))(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06e26-8fa3-414e-a502-8d1c18ba9eb7",
   "metadata": {},
   "source": [
    "#### A *very* minimal training pipeline.\n",
    "\n",
    "Here is some basic training and evaluation code to get you started.\n",
    "\n",
    "**Important**: I cannot stress enough that this is a **terrible** example of how to implement a training pipeline. You can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89e48f-d8f3-4122-842d-1ff389499854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training hyperparameters.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "batch_size = 128\n",
    "\n",
    "# Architecture hyperparameters.\n",
    "input_size = 28*28\n",
    "width = 16\n",
    "depth = 2\n",
    "\n",
    "# Dataloaders.\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=4)\n",
    "dl_val   = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=4)\n",
    "dl_test  = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# Instantiate model and optimizer.\n",
    "model_mlp = MLP([input_size] + [width]*depth + [10]).to(device)\n",
    "opt = torch.optim.Adam(params=model_mlp.parameters(), lr=lr)\n",
    "model_mlp = MyMLP([16, 16]).to(device)\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"dla_lab\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"architecture\": \"MLP\",\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"epochs\": 100,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Training loop.\n",
    "losses_and_accs = []\n",
    "for epoch in range(epochs):\n",
    "    loss = train_epoch(model_mlp, dl_train, opt, epoch, device=device)\n",
    "    (val_acc, _) = evaluate_model(model_mlp, dl_val, device=device)\n",
    "    losses_and_accs.append((loss, val_acc))\n",
    "    wandb.log({\"accuracy\": val_acc, \"loss\": loss})\n",
    "\n",
    "wandb.finish()\n",
    "# And finally plot the curves.\n",
    "plot_validation_curves(losses_and_accs)\n",
    "print(f'Accuracy report on TEST:\\n {evaluate_model(model_mlp, dl_test, device=device)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {},
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "Implement a *simple* Multilayer Perceptron to classify the 10 digits of MNIST (e.g. two *narrow* layers). Use my code above as inspiration, but implement your own training pipeline -- you will need it later. Train this model to convergence, monitoring (at least) the loss and accuracy on the training and validation sets for every epoch. Below I include a basic implementation to get you started -- remember that you should write your *own* pipeline!\n",
    "\n",
    "**Note**: This would be a good time to think about *abstracting* your model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models.\n",
    "\n",
    "**Important**: Given the *many* runs you will need to do, and the need to *compare* performance between them, this would **also** be a great point to study how **Tensorboard** or **Weights and Biases** can be used for performance monitoring.# Your code here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86877777",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19d96405-36e7-4074-803c-fb02576cd528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, widths: list[int], nclasses: int=10) -> None:\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.net = nn.Sequential()\n",
    "        for width in widths:\n",
    "            self.net.append(nn.LazyLinear(width))\n",
    "            self.net.append(nn.ReLU())\n",
    "        self.output = nn.LazyLinear(nclasses)\n",
    "\n",
    "    def forward(self, X):\n",
    "        f = self.flatten(X) # flattened input\n",
    "        h = self.net(f)\n",
    "        o = self.output(h)\n",
    "        return o"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d22a1d9",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b45e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loss_fn, validation_dl):\n",
    "    avg_acc = 0\n",
    "    avg_loss = 0\n",
    "    for (X, y) in tqdm(validation_dl, desc=\"Validation\", leave=False):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        prediction = model(X)\n",
    "        avg_acc += (prediction.argmax(1) == y).sum().item()\n",
    "        avg_loss += loss_fn(prediction, y)\n",
    "        X.detach()\n",
    "        y.detach()\n",
    "    return avg_loss / len(validation_dl.dataset), avg_acc / len(validation_dl.dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "379e5946",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a68790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, loss_fn, optimizer, training_dl, validation_dl, epochs, log=True):\n",
    "    for epoch in range(epochs):\n",
    "        for (X, y) in tqdm(training_dl, desc=f\"Training #{epoch + 1}\", leave=True):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            prediction = model(X)\n",
    "            loss = loss_fn(prediction, y)\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_loss, avg_acc = validate(model, loss_fn, validation_dl)\n",
    "            total_norm = 0\n",
    "            par = [p.to(\"cpu\") for p in model.parameters()]\n",
    "            for p in par:\n",
    "                total_norm += p.norm().item()\n",
    "            if log:\n",
    "                wandb.log({\"loss\": avg_loss, \"acc\": avg_acc, \"grad\": total_norm})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef98944",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "m = MyMLP([16, 16]).to(device)\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"dla_lab\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"architecture\": \"MLP\",\n",
    "    \"dataset\": \"MNIST\",\n",
    "    \"epochs\": 100,\n",
    "    }\n",
    ")\n",
    "training(m, nn.CrossEntropyLoss(), torch.optim.Adam(params=m.parameters(), lr=0.0001), dl_train, dl_val, 100)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8ad9b-e3ae-4c49-9bec-35aaea149b08",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Rinse and Repeat\n",
    "\n",
    "Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs *without* residual connections do not always work better and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Hint**: You probably should do this exercise using CIFAR10, since MNIST is *very* easy (at least up to about 99% accuracy).\n",
    "\n",
    "**Spoiler**: If you plan to do optional exercise 3.3, you should think *very* carefully about the architectures of your CNNs here (so you can reuse them!).### Exercise 1.1: A baseline MLP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f33ce7f1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c8baa0e-b17f-4a77-8a88-dadfdc6763ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "# Your code here.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, output_channels: list[int] = [64, 128, 256], kernel_sizes: list[Union[tuple[int], int]] = [3, 3, 3], \n",
    "                 strides: list[Union[tuple[int], int]] = [1, 1, 1], paddings: list[Union[tuple[int], int]] = [1, 1, 1],\n",
    "                 classifier_head_widths: list[int] = [64, 128, 256], nclasses: int = 10) -> None:\n",
    "        super(CNN, self).__init__()\n",
    "        assert len(kernel_sizes) == len(paddings) == len(strides)\n",
    "        self.net = nn.Sequential()\n",
    "        for i in range(len(kernel_sizes)):\n",
    "            self.net.append(nn.LazyConv2d(output_channels[i], kernel_size=kernel_sizes[i], padding=paddings[i], stride=strides[i]))\n",
    "            self.net.append(nn.ReLU())\n",
    "            self.net.append(nn.MaxPool2d(2, stride=2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.head = nn.Sequential()\n",
    "        for width in classifier_head_widths:\n",
    "            self.head.append(nn.LazyLinear(width))\n",
    "            self.head.append(nn.ReLU())\n",
    "        self.head.append(nn.LazyLinear(nclasses))\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = self.flatten(self.net(X)) # encode image into vector\n",
    "        O = self.head(Z) # classify\n",
    "        return O\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "139c96a3",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "686b1baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load MNIST train and test.\n",
    "ds_train = CIFAR10(root='./datasets', train=True, download=True, transform=ToTensor())\n",
    "ds_test = CIFAR10(root='./datasets', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Split train into train and validation.\n",
    "val_size = 10000\n",
    "I = np.random.permutation(len(ds_train))\n",
    "ds_val = Subset(ds_train, I[:val_size])\n",
    "ds_train = Subset(ds_train, I[val_size:])\n",
    "\n",
    "bs=32\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=bs, shuffle=True)\n",
    "dl_val = DataLoader(ds_val, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf92f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manu/miniconda3/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a688943a4bb42379605ae305d35ef72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666932176666099, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "wandb.init(\"DLA_LAB_01\")\n",
    "training(model, loss_fn, optimizer, dl_train, dl_val, 100)\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "864838d5",
   "metadata": {},
   "source": [
    "### Deeper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9c9d1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manu/miniconda3/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2679szmk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86626e0aae14b498aa68e3cecc08c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">silvery-disco-12</strong> at: <a href='https://wandb.ai/cascetto/dla_labs/runs/2679szmk' target=\"_blank\">https://wandb.ai/cascetto/dla_labs/runs/2679szmk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230312_182914-2679szmk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2679szmk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455106a8add2497896e270809ced1dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668143416670014, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/manu/projects/dla_labs/wandb/run-20230312_183005-u2yscx7j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cascetto/dla_labs/runs/u2yscx7j' target=\"_blank\">cool-bush-13</a></strong> to <a href='https://wandb.ai/cascetto/dla_labs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cascetto/dla_labs' target=\"_blank\">https://wandb.ai/cascetto/dla_labs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cascetto/dla_labs/runs/u2yscx7j' target=\"_blank\">https://wandb.ai/cascetto/dla_labs/runs/u2yscx7j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training #1: 100%|██████████| 1250/1250 [00:10<00:00, 124.18it/s]\n",
      "Training #2: 100%|██████████| 1250/1250 [00:09<00:00, 126.77it/s]\n",
      "Training #3: 100%|██████████| 1250/1250 [00:09<00:00, 126.91it/s]\n",
      "Training #4: 100%|██████████| 1250/1250 [00:09<00:00, 126.32it/s]\n",
      "Training #5: 100%|██████████| 1250/1250 [00:09<00:00, 126.66it/s]\n",
      "Training #6: 100%|██████████| 1250/1250 [00:09<00:00, 126.23it/s]\n",
      "Training #7: 100%|██████████| 1250/1250 [00:09<00:00, 126.31it/s]\n",
      "Training #8: 100%|██████████| 1250/1250 [00:09<00:00, 126.54it/s]\n",
      "Training #9: 100%|██████████| 1250/1250 [00:09<00:00, 126.01it/s]\n",
      "Training #10: 100%|██████████| 1250/1250 [00:09<00:00, 126.35it/s]\n",
      "Training #11: 100%|██████████| 1250/1250 [00:09<00:00, 126.22it/s]\n",
      "Training #12: 100%|██████████| 1250/1250 [00:09<00:00, 125.85it/s]\n",
      "Training #13: 100%|██████████| 1250/1250 [00:09<00:00, 125.95it/s]\n",
      "Training #14: 100%|██████████| 1250/1250 [00:09<00:00, 126.28it/s]\n",
      "Training #15: 100%|██████████| 1250/1250 [00:09<00:00, 126.92it/s]\n",
      "Training #16: 100%|██████████| 1250/1250 [00:09<00:00, 126.33it/s]\n",
      "Training #17: 100%|██████████| 1250/1250 [00:09<00:00, 126.05it/s]\n",
      "Training #18: 100%|██████████| 1250/1250 [00:09<00:00, 126.14it/s]\n",
      "Training #19: 100%|██████████| 1250/1250 [00:09<00:00, 126.18it/s]\n",
      "Training #20: 100%|██████████| 1250/1250 [00:09<00:00, 126.41it/s]\n",
      "Training #21: 100%|██████████| 1250/1250 [00:09<00:00, 126.38it/s]\n",
      "Training #22: 100%|██████████| 1250/1250 [00:09<00:00, 126.40it/s]\n",
      "Training #23: 100%|██████████| 1250/1250 [00:09<00:00, 126.63it/s]\n",
      "Training #24: 100%|██████████| 1250/1250 [00:09<00:00, 127.13it/s]\n",
      "Training #25: 100%|██████████| 1250/1250 [00:09<00:00, 126.63it/s]\n",
      "Training #26: 100%|██████████| 1250/1250 [00:09<00:00, 126.73it/s]\n",
      "Training #27: 100%|██████████| 1250/1250 [00:09<00:00, 126.34it/s]\n",
      "Training #28: 100%|██████████| 1250/1250 [00:09<00:00, 125.92it/s]\n",
      "Training #29: 100%|██████████| 1250/1250 [00:09<00:00, 126.73it/s]\n",
      "Training #30: 100%|██████████| 1250/1250 [00:09<00:00, 127.10it/s]\n",
      "Training #31: 100%|██████████| 1250/1250 [00:09<00:00, 126.71it/s]\n",
      "Training #32: 100%|██████████| 1250/1250 [00:09<00:00, 126.51it/s]\n",
      "Training #33: 100%|██████████| 1250/1250 [00:09<00:00, 126.49it/s]\n",
      "Training #34: 100%|██████████| 1250/1250 [00:09<00:00, 126.42it/s]\n",
      "Training #35: 100%|██████████| 1250/1250 [00:09<00:00, 126.95it/s]\n",
      "Training #36: 100%|██████████| 1250/1250 [00:09<00:00, 127.19it/s]\n",
      "Training #37: 100%|██████████| 1250/1250 [00:09<00:00, 126.67it/s]\n",
      "Training #38: 100%|██████████| 1250/1250 [00:09<00:00, 127.40it/s]\n",
      "Training #39: 100%|██████████| 1250/1250 [00:09<00:00, 126.61it/s]\n",
      "Training #40: 100%|██████████| 1250/1250 [00:09<00:00, 126.15it/s]\n",
      "Training #41: 100%|██████████| 1250/1250 [00:09<00:00, 126.88it/s]\n",
      "Training #42: 100%|██████████| 1250/1250 [00:09<00:00, 126.96it/s]\n",
      "Training #43: 100%|██████████| 1250/1250 [00:09<00:00, 127.09it/s]\n",
      "Training #44: 100%|██████████| 1250/1250 [00:09<00:00, 126.89it/s]\n",
      "Training #45: 100%|██████████| 1250/1250 [00:09<00:00, 126.41it/s]\n",
      "Training #46: 100%|██████████| 1250/1250 [00:09<00:00, 127.51it/s]\n",
      "Training #47: 100%|██████████| 1250/1250 [00:09<00:00, 127.35it/s]\n",
      "Training #48: 100%|██████████| 1250/1250 [00:09<00:00, 127.20it/s]\n",
      "Training #49: 100%|██████████| 1250/1250 [00:09<00:00, 127.46it/s]\n",
      "Training #50: 100%|██████████| 1250/1250 [00:09<00:00, 126.99it/s]\n",
      "Training #51: 100%|██████████| 1250/1250 [00:09<00:00, 126.34it/s]\n",
      "Training #52: 100%|██████████| 1250/1250 [00:09<00:00, 126.02it/s]\n",
      "Training #53: 100%|██████████| 1250/1250 [00:09<00:00, 127.80it/s]\n",
      "Training #54: 100%|██████████| 1250/1250 [00:09<00:00, 136.42it/s]\n",
      "Training #55: 100%|██████████| 1250/1250 [00:09<00:00, 132.74it/s]\n",
      "Training #56: 100%|██████████| 1250/1250 [00:10<00:00, 123.09it/s]\n",
      "Training #57: 100%|██████████| 1250/1250 [00:10<00:00, 124.07it/s]\n",
      "Training #58: 100%|██████████| 1250/1250 [00:10<00:00, 124.93it/s]\n",
      "Training #59: 100%|██████████| 1250/1250 [00:09<00:00, 125.14it/s]\n",
      "Training #60: 100%|██████████| 1250/1250 [00:09<00:00, 125.35it/s]\n",
      "Training #61: 100%|██████████| 1250/1250 [00:10<00:00, 123.55it/s]\n",
      "Training #62: 100%|██████████| 1250/1250 [00:10<00:00, 123.85it/s]\n",
      "Training #63: 100%|██████████| 1250/1250 [00:10<00:00, 124.50it/s]\n",
      "Training #64: 100%|██████████| 1250/1250 [00:09<00:00, 125.10it/s]\n",
      "Training #65: 100%|██████████| 1250/1250 [00:10<00:00, 124.25it/s]\n",
      "Training #66: 100%|██████████| 1250/1250 [00:10<00:00, 124.46it/s]\n",
      "Training #67: 100%|██████████| 1250/1250 [00:10<00:00, 123.81it/s]\n",
      "Training #68: 100%|██████████| 1250/1250 [00:10<00:00, 124.75it/s]\n",
      "Training #69: 100%|██████████| 1250/1250 [00:10<00:00, 124.30it/s]\n",
      "Training #70: 100%|██████████| 1250/1250 [00:10<00:00, 124.76it/s]\n",
      "Training #71: 100%|██████████| 1250/1250 [00:10<00:00, 124.18it/s]\n",
      "Training #72: 100%|██████████| 1250/1250 [00:10<00:00, 124.95it/s]\n",
      "Training #73: 100%|██████████| 1250/1250 [00:10<00:00, 123.91it/s]\n",
      "Training #74: 100%|██████████| 1250/1250 [00:10<00:00, 123.91it/s]\n",
      "Training #75: 100%|██████████| 1250/1250 [00:10<00:00, 124.65it/s]\n",
      "Training #76: 100%|██████████| 1250/1250 [00:10<00:00, 124.26it/s]\n",
      "Training #77: 100%|██████████| 1250/1250 [00:10<00:00, 124.97it/s]\n",
      "Training #78: 100%|██████████| 1250/1250 [00:10<00:00, 124.09it/s]\n",
      "Training #79: 100%|██████████| 1250/1250 [00:09<00:00, 125.11it/s]\n",
      "Training #80: 100%|██████████| 1250/1250 [00:10<00:00, 123.85it/s]\n",
      "Training #81: 100%|██████████| 1250/1250 [00:10<00:00, 124.77it/s]\n",
      "Training #82: 100%|██████████| 1250/1250 [00:10<00:00, 123.92it/s]\n",
      "Training #83: 100%|██████████| 1250/1250 [00:10<00:00, 124.23it/s]\n",
      "Training #84: 100%|██████████| 1250/1250 [00:10<00:00, 124.49it/s]\n",
      "Training #85: 100%|██████████| 1250/1250 [00:10<00:00, 124.86it/s]\n",
      "Training #86: 100%|██████████| 1250/1250 [00:10<00:00, 124.18it/s]\n",
      "Training #87: 100%|██████████| 1250/1250 [00:10<00:00, 123.75it/s]\n",
      "Training #88: 100%|██████████| 1250/1250 [00:10<00:00, 124.56it/s]\n",
      "Training #89: 100%|██████████| 1250/1250 [00:10<00:00, 123.77it/s]\n",
      "Training #90: 100%|██████████| 1250/1250 [00:09<00:00, 125.54it/s]\n",
      "Training #91: 100%|██████████| 1250/1250 [00:09<00:00, 125.04it/s]\n",
      "Training #92: 100%|██████████| 1250/1250 [00:09<00:00, 125.64it/s]\n",
      "Training #93: 100%|██████████| 1250/1250 [00:10<00:00, 124.99it/s]\n",
      "Training #94: 100%|██████████| 1250/1250 [00:09<00:00, 125.31it/s]\n",
      "Training #95: 100%|██████████| 1250/1250 [00:10<00:00, 124.44it/s]\n",
      "Training #96: 100%|██████████| 1250/1250 [00:10<00:00, 124.97it/s]\n",
      "Training #97: 100%|██████████| 1250/1250 [00:09<00:00, 125.16it/s]\n",
      "Training #98: 100%|██████████| 1250/1250 [00:09<00:00, 125.32it/s]\n",
      "Training #99: 100%|██████████| 1250/1250 [00:09<00:00, 125.44it/s]\n",
      "Training #100: 100%|██████████| 1250/1250 [00:10<00:00, 124.49it/s]\n",
      "                                                              \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f262258458c8422b9511b53a53cc73d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "out_channels = [64, 128, 256, 256, 512]\n",
    "kernel_sizes = [3] * 5\n",
    "paddings = [1] * 5\n",
    "strides = [1] * 5\n",
    "model = CNN(kernel_sizes=kernel_sizes, output_channels=out_channels, strides=strides, paddings=paddings).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "wandb.init(\"DLA_LAB_01\")\n",
    "training(model, loss_fn, optimizer, dl_train, dl_val, 100)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {},
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07978e8e-9f2e-4949-9699-495af6cb6349",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Explain why Residual Connections are so effective\n",
    "Use your two models (with and without residual connections) you developed above to study and **quantify** why the residual versions of the networks learn more effectively.\n",
    "\n",
    "**Hint**: A good starting point might be looking at the gradient magnitudes passing through the networks during backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e81a3-08ca-4549-a2f8-f47cf5a0308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Fully-convolutionalize a network.\n",
    "Take one of your trained classifiers and **fully-convolutionalize** it. That is, turn it into a network that can predict classification outputs at *all* pixels in an input image. Can you turn this into a **detector** of handwritten digits? Give it a try.\n",
    "\n",
    "**Hint 1**: Sometimes the process of fully-convolutionalization is called \"network surgery\".\n",
    "\n",
    "**Hint 2**: To test your fully-convolutionalized networks you might want to write some functions to take random MNIST samples and embed them into a larger image (i.e. in a regular grid or at random positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e33c912-0716-44ef-a91b-47ca19a2b2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243f811-8227-4c6f-b07f-56e8cd91643a",
   "metadata": {},
   "source": [
    "### Exercise 2.3: *Explain* the predictions of a CNN\n",
    "\n",
    "Use the CNN model you trained in Exercise 1.2 and implement [*Class Activation Maps*](http://cnnlocalization.csail.mit.edu/#:~:text=A%20class%20activation%20map%20for,decision%20made%20by%20the%20CNN.):\n",
    "\n",
    "> B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "\n",
    "Use your implementation to demonstrate how your trained CNN *attends* to specific image features to recognize *specific* classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634a700-56c2-48fd-96e0-4c94d1bd0cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
